---
language:
- ug
license: mit
task_categories:
- text-generation
- translation
- fill-mask
pretty_name: Uyghur Corpus (AI-Optimized)
homepage: https://huggingface.co/datasets/Uyghur-Corpus/Uyghur-Corpus  # <--- Ø¨Û‡ Ù‚Û‡Ø± Ù‚ÙˆØ´Û‡Ù„Ø¯Ù‰
dataset_info:
  features:
  - name: title
    dtype: string
  - name: text
    dtype: string
  - name: author
    dtype: string
  - name: source
    dtype: string
  - name: date
    dtype: string
  - name: translator
    dtype: string
  config_name: default
  splits:
  - name: train
    num_bytes: 41943040
    num_examples: 25000
---

# ğŸŒŸ Uyghur AI Corpus: Bridging Heritage & Technology
# ğŸŒŸ Ø¦Û‡ÙŠØºÛ‡Ø±Ú†Û• Ø³ÛˆÙ†Ø¦Ù‰ÙŠ Ø¦Ù‰Ø¯Ø±Ø§Ùƒ Ø®Û•Ø²Ù‰Ù†Ù‰Ø³Ù‰: Ù…Ù‰Ø±Ø§Ø³ Û‹Û• ØªÛØ®Ù†Ù‰ÙƒØ§ ÙƒÛ†Û‹Ø±ÛˆÙƒÙ‰

![Status](https://img.shields.io/badge/Status-Actively%20Maintained-success) ![Language](https://img.shields.io/badge/Language-Uyghur-blue) ![Purpose](https://img.shields.io/badge/Purpose-AI%20Training-purple) ![License](https://img.shields.io/badge/License-MIT-green)

## ğŸŒ¹ Introduction / ÙƒÙ‰Ø±Ù‰Ø´ Ø³Û†Ø²

In the era of Artificial Intelligence, language is data, and data is survival. 
**The Uyghur AI Corpus** is a passionate initiative to ensure the Uyghur language thrives in the digital age. This dataset serves as a foundational resource to train Large Language Models (LLMs), enabling them to understand, generate, and translate Uyghur with native-level proficiency.

**Ø³ÛˆÙ†Ø¦Ù‰ÙŠ Ø¦Ù‰Ø¯Ø±Ø§Ùƒ (AI) Ø¯Û•Û‹Ø±Ù‰Ø¯Û•ØŒ ØªÙ‰Ù„ â€” Ø³Ø§Ù†Ù„Ù‰Ù‚ Ù…Û•Ù„Û‡Ù…Ø§Øª Ø¯ÛÙ…Û•ÙƒØªÛ‡Ø±.**
Ø¨Û‡ Ø¦Ø§Ù…Ø¨Ø§Ø± â€” Ø¦Û‡ÙŠØºÛ‡Ø± ØªÙ‰Ù„Ù‰Ù†Ù‰Ú­ Ø±Û•Ù‚Û•Ù…Ù„Ù‰Ùƒ Ø¯Û‡Ù†ÙŠØ§Ø¯Ù‰ÙƒÙ‰ Ø¦ÙˆØ±Ù†Ù‰Ù†Ù‰ Ø³Ø§Ù‚Ù„Ø§Ù¾ Ù‚ÛÙ„Ù‰Ø´ Û‹Û• ØªÛØ®Ù‰Ù…Û‡ ÙŠÛˆÙƒØ³Û•Ù„Ø¯ÛˆØ±ÛˆØ´ Ø¦ÛˆÚ†ÛˆÙ† ØªÛ•ÙŠÙŠØ§Ø±Ù„Ø§Ù†ØºØ§Ù† Ø¨Ù‰Ø± ÙƒÛ†Ú­ÛˆÙ„ Ø³ÙˆÛ‹ØºÙ‰Ø³Ù‰Ø¯Û‡Ø±. Ø¨Ù‰Ø²Ù†Ù‰Ú­ Ù…Û•Ù‚Ø³Ù‰ØªÙ‰Ù…Ù‰Ø²: ÙƒÛ•Ù„Ú¯ÛˆØ³Ù‰Ø¯Ù‰ÙƒÙ‰ Ø³ÛˆÙ†Ø¦Ù‰ÙŠ Ø¦Ù‰Ø¯Ø±Ø§Ùƒ Ù…ÙˆØ¯ÛÙ„Ù„Ù‰Ø±Ù‰Ù†Ù‰Ú­ (ChatGPT, Claude, Gemini Ù‚Ø§ØªØ§Ø±Ù„Ù‰Ù‚) Ø¦Û‡ÙŠØºÛ‡Ø±Ú†Ù‰Ù†Ù‰ Ø±Ø§Û‹Ø§Ù† Ú†ÛˆØ´Ù‰Ù†Ù‰Ø´Ù‰ØŒ ØªÛ•Ø±Ø¬Ù‰Ù…Û• Ù‚Ù‰Ù„Ù‰Ø´Ù‰ Û‹Û• Ø¨Ù‰Ø²Ù†Ù‰Ú­ Ù…Û•Ø¯Û•Ù†Ù‰ÙŠÙ‰ØªÙ‰Ù…Ù‰Ø²Ù†Ù‰ ØªÙˆØºØ±Ø§ Ø¦Ù‰Ù¾Ø§Ø¯Ù‰Ù„Ù‰Ø´Ù‰Ú¯Û• Ø¨Ù‰Ø± ØªØ§Ù…Ú†Û• Ø¨ÙˆÙ„Ø³Ù‰Ù…Û‡ Ú¾Û•Ø³Ø³Û• Ù‚ÙˆØ´Û‡Ø´ØªÛ‡Ø±.

---

## ğŸ’ Source & Collection / Ù…Û•Ù†Ø¨Û• Û‹Û• ØªÙˆÙ¾Ù„Ù‰Ù†Ù‰Ø´Ù‰

This corpus is a carefully curated collection of texts sourced from the open internet. It represents the collective intellectual heritage of the Uyghur people shared on various public platforms, forums, and websites over the years.

* **Diverse Origins:** Stories, essays, articles, and historical accounts available in the public domain.
* **Respect for Authors:** While collected for AI training purposes, we deeply respect the original creators. Metadata such as `author` and `source` has been preserved wherever possible to credit the intellectual owners.
* **Cleaned & Processed:** The raw web data has been meticulously cleaned, formatted, and structured to meet high-quality AI training standards.

**Ø¨Û‡ Ø®Û•Ø²Ù‰Ù†Ù‰Ù†Ù‰Ú­ Ù…Û•Ù†Ø¨Û•Ø³Ù‰ â€” ÙƒÛ•Ú­ Ø¦Ù‰Ù†ØªÛØ±Ù†ÛØª Ø¯Û‡Ù†ÙŠØ§Ø³Ù‰Ø¯Û‡Ø±.**
Ø¨Û‡ Ø¦Ø§Ù…Ø¨Ø§Ø±Ø¯Ù‰ÙƒÙ‰ Ø¦Û•Ø³Û•Ø±Ù„Û•Ø± ÙŠÙ‰Ù„Ù„Ø§Ø±Ø¯Ù‰Ù† Ø¨Û‡ÙŠØ§Ù† ØªÛˆØ±Ù„ÛˆÙƒ ØªÙˆØ± Ø¨Û•ØªØŒ Ù…Û‡Ù†Ø¨Û•Ø± Û‹Û• Ø¦Ù‰Ø¬ØªÙ‰Ù…Ø§Ø¦Ù‰ÙŠ ØªØ§Ø±Ø§ØªÙ‚Û‡Ù„Ø§Ø±Ø¯Ø§ Ø¦ÛÙ„Ø§Ù† Ù‚Ù‰Ù„Ù‰Ù†ØºØ§Ù†ØŒ Ø®Û•Ù„Ù‚Ù‰Ù…Ù‰Ø²Ù†Ù‰Ú­ Ø¦Û•Ù‚Ù„Ù‰ÙŠ Ø¨Ø§ÙŠÙ„Ù‰Ù‚Ù‰ Ø¨ÙˆÙ„ØºØ§Ù† Ø¦ÙˆÚ†Û‡Ù‚ Ù…Û•Ù†Ø¨Û•Ù„Ù‰Ùƒ Ø¦Û•Ø³Û•Ø±Ù„Û•Ø±Ø¯Ù‰Ù† ØªØ§Ù„Ù„Ø§Ù¾ ÙŠÙ‰ØºÙ‰Ù„Ø¯Ù‰.

* **Ù…Û•Ù‚Ø³Û•Øª:** Ø¨Û‡ Ø¦Û•Ø³Û•Ø±Ù„Û•Ø±Ù†Ù‰ ØªÙˆÙ¾Ù„Ø§Ø´ØªÙ‰ÙƒÙ‰ Ø¨Ù‰Ø±Ø¯Ù‰Ù†Ø¨Ù‰Ø± Ù…Û•Ù‚Ø³Û•Øª â€” **Ø³ÛˆÙ†Ø¦Ù‰ÙŠ Ø¦Ù‰Ø¯Ø±Ø§ÙƒÙ†Ù‰Ú­ Ø¦Û‡ÙŠØºÛ‡Ø±Ú†Û• Ø³Û•Û‹Ù‰ÙŠÛ•Ø³Ù‰Ù†Ù‰ Ø¦Û†Ø³ØªÛˆØ±ÛˆØ´**ØŒ ØªÙ‰Ù„Ù‰Ù…Ù‰Ø²Ù†Ù‰Ú­ Ù†Ø§Ø²Û‡ÙƒÙ„Ù‰Ù‚Ù‰ Û‹Û• Ù¾Ø§Ø³Ø§Ú¾Ù‰ØªÙ‰Ù†Ù‰ Ù…Ø§Ø´Ù‰Ù†Ù‰Ù„Ø§Ø±ØºØ§ Ø¦Û†Ú¯Ù‰ØªÙ‰Ø´ØªÙ‰Ù† Ø¦Ù‰Ø¨Ø§Ø±Û•Øª.
* **Ú¾Û†Ø±Ù…Û•Øª:** Ø¨Ù‰Ø² Ú¾Û•Ø± Ø¨Ù‰Ø± ÙŠØ§Ø²Ù…Ù‰Ù†Ù‰Ú­ Ø¦Ø§Ù¾ØªÙˆØ±Ù‰ØºØ§ Û‹Û• Ø¦Û•Ø³Ù„Ù‰ Ù…Û•Ù†Ø¨Û•Ø³Ù‰Ú¯Û• Ø¦Ø§Ù„Ù‰ÙŠ Ú¾Û†Ø±Ù…Û•Øª Ø¨Ù‰Ù„Ø¯ÛˆØ±Ù‰Ù…Ù‰Ø². Ø´Û‡Ú­Ø§ØŒ Ø³Ø§Ù†Ù„Ù‰Ù‚ Ù…Û•Ù„Û‡Ù…Ø§ØªÙ„Ø§Ø± ØªØ§Ø²Ù‰Ù„Ø§Ù†ØºØ§Ù†Ø¯Ø§ `author` (Ø¦Ø§Ù¾ØªÙˆØ±) Û‹Û• `source` (Ù…Û•Ù†Ø¨Û•) Ø¦Û‡Ú†Û‡Ø±Ù„Ù‰Ø±Ù‰ Ø¦Ù‰Ù…ÙƒØ§Ù†Ù‚Û•Ø¯Û•Ø± Ø³Ø§Ù‚Ù„Ø§Ù¾ Ù‚ÛÙ„Ù‰Ù†Ø¯Ù‰.

---

## ğŸš€ Technical Highlights / ØªÛØ®Ù†Ù‰ÙƒÙ‰Ù„Ù‰Ù‚ Ø¦Ø§Ù„Ø§Ú¾Ù‰Ø¯Ù‰Ù„Ù‰ÙƒÙ‰

To solve the "Lost-in-the-Middle" problem common in LLM training, this dataset features **Semantic Chunking**:

LLM Ù…Û•Ø´Ù‰Ù‚Ù‰Ø¯Ù‰ÙƒÙ‰ Â«Ø¦Û‡Ø²Û‡Ù† Ù…Û•Ø²Ù…Û‡Ù†Ù†Ù‰ Ø¦Û‡Ù†ØªÛ‡Ù¾ Ù‚ÛÙ„Ù‰Ø´Â» Ù…Û•Ø³Ù‰Ù„Ù‰Ø³Ù‰Ù†Ù‰ Ú¾Û•Ù„ Ù‚Ù‰Ù„Ù‰Ø´ Ø¦ÛˆÚ†ÛˆÙ†ØŒ Ø¨Û‡ Ø¦Ø§Ù…Ø¨Ø§Ø± **Â«Ù…Û•Ø²Ù…Û‡Ù†Ù„Û‡Ù‚ Ø¨Û†Ù„Û•ÙƒÂ» (Semantic Chunking)** ØªÛØ®Ù†Ù‰ÙƒÙ‰Ø³Ù‰ Ø¨Ù‰Ù„Û•Ù† Ø¨Ù‰Ø± ØªÛ•Ø±Û•Ù¾ Ù‚Ù‰Ù„Ù‰Ù†Ø¯Ù‰:

1.  **Format:** `Parquet` (Fast, compressed, and ready for Python Pandas/Hugging Face).
2.  **Chunking Strategy:** Long texts (novels, long essays) are intelligently split into 2000-word segments without breaking sentences or paragraphs.
3.  **Schema Compatibility:** Renamed standard columns (e.g., `content` â†’ `text`) for instant compatibility with PyTorch/TensorFlow datasets.

**Ø¦Û•Ø³ÙƒÛ•Ø±ØªÙ‰Ø´:** Ø¦Û‡Ø²Û‡Ù† Ø±ÙˆÙ…Ø§Ù†Ù„Ø§Ø± Û‹Û• Ú†ÙˆÚ­ Ø¦Û•Ø³Û•Ø±Ù„Û•Ø± Ø¦ÙˆÙ‚Û‡Ø±Ù…Û•Ù†Ù„Û•Ø±Ú¯Û• Û‹Û• Ù…Ø§Ø´Ù‰Ù†Ù‰ØºØ§ Ù‚Û‡Ù„Ø§ÙŠÙ„Ù‰Ù‚ Ø¨ÙˆÙ„Û‡Ø´Ù‰ Ø¦ÛˆÚ†ÛˆÙ† `(1-Ù‚Ù‰Ø³Ù‰Ù…)`ØŒ `(2-Ù‚Ù‰Ø³Ù‰Ù…)` Ø´Û•ÙƒÙ„Ù‰Ø¯Û• Ø±Û•ØªÙ„Ù‰Ùƒ Ø¨Û†Ù„ÛˆÙ†Ø¯Ù‰.

---

## ğŸ“‚ Data Structure / Ù‚Û‡Ø±Û‡Ù„Ù…Ù‰Ø³Ù‰

| Column / Ø¦Ù‰Ø³ØªÙˆÙ† | Meaning / Ù…Û•Ù†Ù‰Ø³Ù‰ |
| :--- | :--- |
| **`title`** | The title of the work (includes part numbers for split texts). <br> Ø¦Û•Ø³Û•Ø± Ù…Ø§Û‹Ø²Û‡Ø³Ù‰. |
| **`text`** | **The main content** used for training. <br> Ù…Û•Ø´Ù‰Ù‚ Ù‚Ù‰Ù„Ø¯Û‡Ø±Û‡Ù„Ù‰Ø¯Ù‰ØºØ§Ù† Ø¦Ø§Ø³Ø§Ø³Ù„Ù‰Ù‚ ØªÛÙƒÙ‰Ø³Øª. |
| **`author`** | The original creator of the work. <br> Ø¦Û•Ø³Û•Ø±Ù†Ù‰Ú­ Ø¦Ø§Ù¾ØªÙˆØ±Ù‰. |
| **`source`** | The origin platform or publisher. <br> Ø¦Û•Ø³Û•Ø± Ø¦ÛÙ„Ù‰Ù†ØºØ§Ù† Ù…Û•Ù†Ø¨Û•. |
| **`date`** | Publication date (if available). <br> Ø¦ÛÙ„Ø§Ù† Ù‚Ù‰Ù„Ù‰Ù†ØºØ§Ù† Û‹Ø§Ù‚ØªÙ‰. |
| **`translator`** | Name of the translator (if applicable). <br> ØªÛ•Ø±Ø¬Ù‰Ù…Ø§Ù† (ØªÛ•Ø±Ø¬Ù‰Ù…Û• Ø¦Û•Ø³Û•Ø±Ù„Û•Ø± Ø¦ÛˆÚ†ÛˆÙ†). |

---

## ğŸ’» Usage Example / Ø¦Ù‰Ø´Ù„Ù‰ØªÙ‰Ø´

You can load this dataset directly in Python using the `datasets` library:

```python
from datasets import load_dataset

# Load the dataset
dataset = load_dataset("Uyghur-Corpus/Uyghur-Corpus")

# Peek at the first entry
print(dataset['train'][0])
