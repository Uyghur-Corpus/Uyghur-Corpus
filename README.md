---
language:
  - ug
license: mit
task_categories:
  - text-generation
  - translation
  - fill-mask
pretty_name: Uyghur Corpus (AI-Optimized)
homepage: https://huggingface.co/datasets/Uyghur-Corpus/Uyghur-Corpus
dataset_info:
  features:
    - name: title
      dtype: string
    - name: text
      dtype: string
    - name: author
      dtype: string
    - name: source
      dtype: string
    - name: date
      dtype: string
    - name: translator
      dtype: string
---

---
language:
  - ug
license: mit
task_categories:
  - text-generation
  - translation
  - fill-mask
pretty_name: Uyghur Corpus (AI-Optimized)
homepage: https://huggingface.co/datasets/Uyghur-Corpus/Uyghur-Corpus
dataset_info:
  features:
    - name: title
      dtype: string
    - name: text
      dtype: string
    - name: author
      dtype: string
    - name: source
      dtype: string
    - name: date
      dtype: string
    - name: translator
      dtype: string
---

# ğŸŒŸ Uyghur AI Corpus: Bridging Heritage & Technology
# ğŸŒŸ Ø¦Û‡ÙŠØºÛ‡Ø±Ú†Û• Ø³ÛˆÙ†Ø¦Ù‰ÙŠ Ø¦Ù‰Ø¯Ø±Ø§Ùƒ Ø®Û•Ø²Ù‰Ù†Ù‰Ø³Ù‰: Ù…Ù‰Ø±Ø§Ø³ Û‹Û• ØªÛØ®Ù†Ù‰ÙƒØ§ ÙƒÛ†Û‹Ø±ÛˆÙƒÙ‰

![Status](https://img.shields.io/badge/Status-Actively%20Maintained-success) ![Language](https://img.shields.io/badge/Language-Uyghur-blue) ![Purpose](https://img.shields.io/badge/Purpose-AI%20Training-purple) ![License](https://img.shields.io/badge/License-MIT-green)

## ğŸŒ¹ Introduction / ÙƒÙ‰Ø±Ù‰Ø´ Ø³Û†Ø²

In the era of Artificial Intelligence, language is data, and data is survival. 
**The Uyghur AI Corpus** is an initiative to ensure the Uyghur language thrives in the digital age. This dataset serves as a foundational resource to train Large Language Models (LLMs), enabling them to understand, generate, and translate Uyghur with native-level proficiency.

**Ø³ÛˆÙ†Ø¦Ù‰ÙŠ Ø¦Ù‰Ø¯Ø±Ø§Ùƒ (AI) Ø¯Û•Û‹Ø±Ù‰Ø¯Û•ØŒ ØªÙ‰Ù„ â€” Ø³Ø§Ù†Ù„Ù‰Ù‚ Ù…Û•Ù„Û‡Ù…Ø§Øª Ø¯ÛÙ…Û•ÙƒØªÛ‡Ø±.**
Ø¨Û‡ Ø¦Ø§Ù…Ø¨Ø§Ø± â€” Ø¦Û‡ÙŠØºÛ‡Ø± ØªÙ‰Ù„Ù‰Ù†Ù‰Ú­ Ø±Û•Ù‚Û•Ù…Ù„Ù‰Ùƒ Ø¯Û‡Ù†ÙŠØ§Ø¯Ù‰ÙƒÙ‰ Ø¦ÙˆØ±Ù†Ù‰Ù†Ù‰ Ø³Ø§Ù‚Ù„Ø§Ù¾ Ù‚ÛÙ„Ù‰Ø´ Û‹Û• ØªÛØ®Ù‰Ù…Û‡ ÙŠÛˆÙƒØ³Û•Ù„Ø¯ÛˆØ±ÛˆØ´ Ø¦ÛˆÚ†ÛˆÙ† ØªÛ•ÙŠÙŠØ§Ø±Ù„Ø§Ù†ØºØ§Ù† Ø¨Ù‰Ø± ÙƒÛ†Ú­ÛˆÙ„ Ø³ÙˆÛ‹ØºÙ‰Ø³Ù‰Ø¯Û‡Ø±. Ø¨Ù‰Ø²Ù†Ù‰Ú­ Ù…Û•Ù‚Ø³Ù‰ØªÙ‰Ù…Ù‰Ø²: ÙƒÛ•Ù„Ú¯ÛˆØ³Ù‰Ø¯Ù‰ÙƒÙ‰ Ø³ÛˆÙ†Ø¦Ù‰ÙŠ Ø¦Ù‰Ø¯Ø±Ø§Ùƒ Ù…ÙˆØ¯ÛÙ„Ù„Ù‰Ø±Ù‰Ù†Ù‰Ú­ Ø¦Û‡ÙŠØºÛ‡Ø±Ú†Ù‰Ù†Ù‰ Ø±Ø§Û‹Ø§Ù† Ú†ÛˆØ´Ù‰Ù†Ù‰Ø´Ù‰ØŒ ØªÛ•Ø±Ø¬Ù‰Ù…Û• Ù‚Ù‰Ù„Ù‰Ø´Ù‰ Û‹Û• Ø¨Ù‰Ø²Ù†Ù‰Ú­ Ù…Û•Ø¯Û•Ù†Ù‰ÙŠÙ‰ØªÙ‰Ù…Ù‰Ø²Ù†Ù‰ ØªÙˆØºØ±Ø§ Ø¦Ù‰Ù¾Ø§Ø¯Ù‰Ù„Ù‰Ø´Ù‰Ú¯Û• Ú¾Û•Ø³Ø³Û• Ù‚ÙˆØ´Û‡Ø´ØªÛ‡Ø±.

---

## ğŸ†• What's New (Feb 2026 Update) / ÙŠÛÚ­Ù‰Ù„Ù‰Ù†Ù‰Ø´Ù„Ø§Ø±

- **Content Update:** Added new Uyghur articles and poetry to the dataset.
- **Ù…Û•Ø²Ù…Û‡Ù† ÙŠÛÚ­Ù‰Ù„Ø§Ù†Ø¯Ù‰:** Ø¦Ø§Ù…Ø¨Ø§Ø±ØºØ§ ÙŠÛÚ­Ù‰Ø¯Ù‰Ù† Ù†Û‡Ø±ØºÛ‡Ù† Ù…Ø§Ù‚Ø§Ù„Ù‰Ù„Û•Ø± Û‹Û• Ø´ÛØ¦Ù‰Ø±Ù„Ø§Ø± Ù‚ÙˆØ´Û‡Ù„Ø¯Ù‰.

---

## ğŸ’ Source & Collection / Ù…Û•Ù†Ø¨Û• Û‹Û• ØªÙˆÙ¾Ù„Ù‰Ù†Ù‰Ø´Ù‰

This corpus is a carefully curated collection of texts sourced from the open internet. Metadata such as `author` and `source` has been preserved wherever possible to credit the intellectual owners.

**Ø¨Û‡ Ø®Û•Ø²Ù‰Ù†Ù‰Ù†Ù‰Ú­ Ù…Û•Ù†Ø¨Û•Ø³Ù‰ â€” ÙƒÛ•Ú­ Ø¦Ù‰Ù†ØªÛØ±Ù†ÛØª Ø¯Û‡Ù†ÙŠØ§Ø³Ù‰Ø¯Û‡Ø±.**
Ø¨Û‡ Ø¦Ø§Ù…Ø¨Ø§Ø±Ø¯Ù‰ÙƒÙ‰ Ø¦Û•Ø³Û•Ø±Ù„Û•Ø± ÙŠÙ‰Ù„Ù„Ø§Ø±Ø¯Ù‰Ù† Ø¨Û‡ÙŠØ§Ù† ØªÙˆØ± Ø¨Û•ØªØŒ Ù…Û‡Ù†Ø¨Û•Ø± Û‹Û• Ø¦Ù‰Ø¬ØªÙ‰Ù…Ø§Ø¦Ù‰ÙŠ ØªØ§Ø±Ø§ØªÙ‚Û‡Ù„Ø§Ø±Ø¯Ø§ Ø¦ÛÙ„Ø§Ù† Ù‚Ù‰Ù„Ù‰Ù†ØºØ§Ù†ØŒ Ø®Û•Ù„Ù‚Ù‰Ù…Ù‰Ø²Ù†Ù‰Ú­ Ø¦Û•Ù‚Ù„Ù‰ÙŠ Ø¨Ø§ÙŠÙ„Ù‰Ù‚Ù‰ Ø¨ÙˆÙ„ØºØ§Ù† Ø¦ÙˆÚ†Û‡Ù‚ Ù…Û•Ù†Ø¨Û•Ù„Ù‰Ùƒ Ø¦Û•Ø³Û•Ø±Ù„Û•Ø±Ø¯Ù‰Ù† ØªØ§Ù„Ù„Ø§Ù¾ ÙŠÙ‰ØºÙ‰Ù„Ø¯Ù‰.

* **Diversity:** Stories, essays, articles, and historical accounts.
* **Respect:** Metadata is preserved to credit original authors.
* **Ù…Û•Ù‚Ø³Û•Øª:** Ø³ÛˆÙ†Ø¦Ù‰ÙŠ Ø¦Ù‰Ø¯Ø±Ø§ÙƒÙ†Ù‰Ú­ Ø¦Û‡ÙŠØºÛ‡Ø±Ú†Û• Ø³Û•Û‹Ù‰ÙŠÛ•Ø³Ù‰Ù†Ù‰ Ø¦Û†Ø³ØªÛˆØ±ÛˆØ´.
* **Ú¾Û†Ø±Ù…Û•Øª:** Ø¦Ø§Ù¾ØªÙˆØ± Û‹Û• Ù…Û•Ù†Ø¨Û• Ø¦Û‡Ú†Û‡Ø±Ù„Ù‰Ø±Ù‰ Ø¦Ù‰Ù…ÙƒØ§Ù†Ù‚Û•Ø¯Û•Ø± Ø³Ø§Ù‚Ù„Ø§Ù¾ Ù‚ÛÙ„Ù‰Ù†Ø¯Ù‰.

---

## ğŸš€ Technical Highlights / ØªÛØ®Ù†Ù‰ÙƒÙ‰Ù„Ù‰Ù‚ Ø¦Ø§Ù„Ø§Ú¾Ù‰Ø¯Ù‰Ù„Ù‰ÙƒÙ‰

To solve the "Lost-in-the-Middle" problem common in LLM training, this dataset features **Semantic Chunking**:



1. **Format:** `Parquet` (Fast, compressed, and ready for Python Pandas/Hugging Face).
2. **Chunking Strategy:** Long texts are intelligently split into 2000-word segments without breaking sentences.
3. **Compatibility:** Standardized columns for instant use with PyTorch/TensorFlow datasets.

---

## ğŸ“‚ Data Structure / Ù‚Û‡Ø±Û‡Ù„Ù…Ù‰Ø³Ù‰

| Column / Ø¦Ù‰Ø³ØªÙˆÙ† | Meaning / Ù…Û•Ù†Ù‰Ø³Ù‰ |
| :--- | :--- |
| **`title`** | The title of the work. / Ø¦Û•Ø³Û•Ø± Ù…Ø§Û‹Ø²Û‡Ø³Ù‰. |
| **`text`** | **The main content** used for training. / Ø¦Ø§Ø³Ø§Ø³Ù„Ù‰Ù‚ ØªÛÙƒÙ‰Ø³Øª. |
| **`author`** | The original creator. / Ø¦Û•Ø³Û•Ø±Ù†Ù‰Ú­ Ø¦Ø§Ù¾ØªÙˆØ±Ù‰. |
| **`source`** | The origin platform. / Ø¦Û•Ø³Û•Ø± Ø¦ÛÙ„Ù‰Ù†ØºØ§Ù† Ù…Û•Ù†Ø¨Û•. |
| **`date`** | Publication date. / Ø¦ÛÙ„Ø§Ù† Ù‚Ù‰Ù„Ù‰Ù†ØºØ§Ù† Û‹Ø§Ù‚ØªÙ‰. |
| **`translator`** | Name of the translator. / ØªÛ•Ø±Ø¬Ù‰Ù…Ø§Ù†. |

---

## ğŸ’» Usage Example / Ø¦Ù‰Ø´Ù„Ù‰ØªÙ‰Ø´ Ø¦ÛˆÙ„Ú¯Ù‰Ø³Ù‰

You can load this dataset directly in Python using the `datasets` library:

```python
from datasets import load_dataset

# Load the dataset
dataset = load_dataset("Uyghur-Corpus/Uyghur-Corpus")

# Peek at the first entry
print(dataset['train'][0])
