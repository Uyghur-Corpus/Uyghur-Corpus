name: Global Parquet Converter & Sync

on:
  push:
    branches: [ "main", "master" ]
  workflow_dispatch:

jobs:
  convert-and-sync:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      
    steps:
      - name: 1. كودلارنى يۈكلەش
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          lfs: true

      - name: 2. Python تەييارلاش
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: 3. زۆرۈر قوراللارنى قاچىلاش
        run: pip install pandas pyarrow fastparquet PyYAML huggingface_hub

      - name: 4. مەلۇماتلارنى جەملەش ۋە ماس قەدەملەش
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python -c "
          import pandas as pd
          import os
          import glob
          import yaml
          import json
          import re
          from collections import Counter, OrderedDict

          def get_auto_tags(text):
              words = re.findall(r'[\u0600-\u06FF]{4,}', text)
              stop_words = {'بىلەن', 'ئۈچۈن', 'دېگەن', 'بولسا', 'بولۇپ', 'شۇنداق', 'ئەگەر', 'ئارقىلىق'}
              important_words = [w for w in words if w not in stop_words]
              return [tag[0] for tag in Counter(important_words).most_common(6)]

          def extract_meta(text):
              date_match = re.search(r'\d{4}[-./]\d{1,2}[-./]\d{1,2}|\d{4}-يىلى', text)
              source_match = re.search(r'(?:مەنبە|Source)\s*[:：]\s*(.*)', text, re.I)
              return (date_match.group(0) if date_match else None, source_match.group(1).strip() if source_match else None)

          # Articles ئىچىدىكى ماقالىلەرنى تېپىش
          md_files = sorted(glob.glob('Articles/**/*.md', recursive=True))
          all_parts = []

          for f in md_files:
              try:
                  with open(f, 'r', encoding='utf-8') as file:
                      content = file.read()
                      name_only = os.path.splitext(os.path.basename(f))[0]
                      num_match = re.search(r'(\d+)$', name_only)
                      base_name = re.sub(r'\s*\d+$', '', name_only).strip()
                      part_num = int(num_match.group(1)) if num_match else 1
                      
                      lines = [l.strip() for l in content.split('\n') if l.strip()]
                      if not lines: continue
                      
                      author = 'Unknown'
                      if len(lines) > 2 and len(lines[-1]) < 40: author = lines[-1]
                      
                      date, source = extract_meta(content)
                      body = '\n'.join(lines[1:-1]) if author != 'Unknown' else '\n'.join(lines[1:])
                      tags = get_auto_tags(body)
                      
                      node = OrderedDict([
                          ('title', lines[0]),
                          ('author', author),
                          ('date', date),
                          ('source', source),
                          ('tags', tags),
                          ('content', f'ئەتىكەتلەر: {chr(44).join(tags)}\n\n{body}'),
                          ('language', 'ug'),
                          ('_base', base_name),
                          ('_part', part_num)
                      ])
                      all_parts.append(node)
              except Exception as e: print(f'Error {f}: {e}')

          # قىسىملىق ماقالىلەرنى بىرىكتۈرۈش
          final_data = []
          skip = set()
          for i in range(len(all_parts)):
              if i in skip: continue
              curr = all_parts[i]
              sections = []
              for j in range(i + 1, len(all_parts)):
                  if all_parts[j]['_base'] == curr['_base']:
                      sections.append(all_parts[j])
                      skip.add(j)
                  else: break
              
              if sections:
                  full_sec = [curr] + sections
                  curr['sections'] = full_sec
                  curr['content'] = '\n\n---\n\n'.join([s['content'] for s in full_sec])
              
              final_node = OrderedDict([(k, v) for k, v in curr.items() if not k.startswith('_') and v is not None])
              final_data.append(final_node)

          if final_data:
              df = pd.DataFrame(final_data)
              df.to_parquet('articles_corpus.parquet', engine='pyarrow', index=False)
              print('✅ Parquet يېڭىلاندى.')

          # Hugging Face غا ماس قەدەملەش
          from huggingface_hub import HfApi
          api = HfApi()
          api.upload_folder(
              folder_path='.',
              repo_id='Uyghur-Corpus/Uyghur-Corpus',
              repo_type='dataset',
              token='${{ secrets.HF_TOKEN }}',
              delete_patterns='*',
              commit_message='Sync including deletions'
          )
          "

      - name: 5. GitHub غا يوللاش
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add articles_corpus.parquet
          git commit -m "Auto-update parquet" || echo "No changes"
          git push
