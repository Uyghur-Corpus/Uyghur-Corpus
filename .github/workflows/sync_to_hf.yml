name: Global Parquet Converter & Sync

on:
  push:
    branches: [ "main", "master" ]
  workflow_dispatch:

jobs:
  convert-and-sync:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      
    steps:
      - name: 1. كودلارنى يۈكلەش
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          lfs: true

      - name: 2. Python تەييارلاش
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: 3. زۆرۈر قوراللارنى قاچىلاش
        run: |
          pip install --upgrade pip
          pip install pandas pyarrow fastparquet PyYAML huggingface_hub

      - name: 4. مەلۇماتلارنى جەملەش ۋە ماس قەدەملەش
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python -c "
          import pandas as pd
          import os
          import glob
          import yaml
          import json
          import re
          from collections import Counter, OrderedDict
          import copy

          def get_auto_tags(text):
              words = re.findall(r'[\u0600-\u06FF]{4,}', text)
              stop_words = {'بىلەن', 'ئۈچۈن', 'دېگەن', 'بولسا', 'بولۇپ', 'شۇنداق', 'ئەگەر', 'ئارقىلىق'}
              important_words = [w for w in words if w not in stop_words]
              return [tag[0] for tag in Counter(important_words).most_common(6)]

          def extract_meta(text):
              date_match = re.search(r'\d{4}[-./]\d{1,2}[-./]\d{1,2}|\d{4}-يىلى', text)
              source_match = re.search(r'(?:مەنبە|Source)\s*[:：]\s*(.*)', text, re.I)
              return (date_match.group(0) if date_match else None, source_match.group(1).strip() if source_match else None)

          md_files = sorted(glob.glob('Articles/**/*.md', recursive=True))
          all_parts = []

          for f in md_files:
              try:
                  with open(f, 'r', encoding='utf-8') as file:
                      content = file.read()
                      name_only = os.path.splitext(os.path.basename(f))[0]
                      num_match = re.search(r'(\d+)$', name_only)
                      base_name = re.sub(r'\s*\d+$', '', name_only).strip()
                      part_num = int(num_match.group(1)) if num_match else 1
                      lines = [l.strip() for l in content.split('\n') if l.strip()]
                      if not lines: continue
                      
                      author = 'Unknown'
                      if len(lines) > 2 and len(lines[-1]) < 40: author = lines[-1]
                      
                      date, source = extract_meta(content)
                      body = '\n'.join(lines[1:-1]) if author != 'Unknown' else '\n'.join(lines[1:])
                      tags = get_auto_tags(body)
                      
                      node = OrderedDict([
                          ('title', lines[0]),
                          ('author', author),
                          ('date', date),
                          ('source', source),
                          ('tags', tags),
                          ('content', f'ئەتىكەتلەر: {chr(44).join(tags)}\n\n{body}'),
                          ('language', 'ug'),
                          ('_base', base_name),
                          ('_part', part_num)
                      ])
                      all_parts.append(node)
              except Exception as e: print(f'Error {f}: {e}')

          final_data = []
          skip = set()
          for i in range(len(all_parts)):
              if i in skip: continue
              curr = all_parts[i]
              
              sections = []
              for j in range(i + 1, len(all_parts)):
                  if all_parts[j]['_base'] == curr['_base']:
                      # Circular reference تۈزەتكەن جاي: ئالدى بىلەن نۇسخىسىنى ئالىمىز
                      sec_node = copy.deepcopy(all_parts[j])
                      # يوشۇرۇن ئۇلاش ئۇچۇرلىرىنى ئۆچۈرىمىز
                      sec_node.pop('_base', None)
                      sec_node.pop('_part', None)
                      sections.append(sec_node)
                      skip.add(j)
                  else: break
              
              # ئاخىرقى نۇسخىنى رەتلەش
              output_node = copy.deepcopy(curr)
              output_node.pop('_base', None)
              output_node.pop('_part', None)

              if sections:
                  # تۇنجى قىسىم ئۇچۇرىنى رەتلەپ، باشقا قىسىملار بىلەن ئۇلايمىز
                  first_part = copy.deepcopy(output_node)
                  output_node['sections'] = [first_part] + sections
                  output_node['content'] = '\n\n---\n\n'.join([s['content'] for s in output_node['sections']])
                  output_node['tags'] = get_auto_tags(output_node['content'])

              final_data.append(output_node)

          if final_data:
              df = pd.DataFrame(final_data)
              # pyarrow يىمىرىلىش مەسىلىسىنى يەنىلا ئالدىنى ئېلىش ئۈچۈن پارامېتىر قوشتۇق
              df.to_parquet('articles_corpus.parquet', engine='pyarrow', index=False)
              print('✅ Parquet updated successfully.')

          from huggingface_hub import HfApi
          api = HfApi()
          api.upload_folder(
              folder_path='.',
              repo_id='Uyghur-Corpus/Uyghur-Corpus',
              repo_type='dataset',
              token='${{ secrets.HF_TOKEN }}',
              delete_patterns='*',
              commit_message='Sync including deletions'
          )
          "

      - name: 5. GitHub غا يوللاش
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add articles_corpus.parquet || echo 'No parquet to add'
          git commit -m "Auto-update parquet" || echo "No changes"
          git push
